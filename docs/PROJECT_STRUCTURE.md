# Egyptian Exchange Pipeline - Project Structure

Complete directory structure and file organization.

## Root Directory

```
Egyptian-Exchange-Market-Data-Pipline/
â”œâ”€â”€ airflow/                           # Airflow DAGs and configuration
â”œâ”€â”€ docs/                              # Documentation
â”œâ”€â”€ egx_dw/                            # dbt project for data warehouse
â”œâ”€â”€ extract/                           # Data ingestion scripts
â”œâ”€â”€ iam/                               # AWS IAM and security setup
â”œâ”€â”€ infrastructure/                    # Docker and service configuration
â”œâ”€â”€ scripts/                           # Utility and automation scripts
â”œâ”€â”€ sql/                               # Database schemas and setup
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ requirements.txt                   # Python dependencies
â””â”€â”€ Makefile                           # Build automation
```

## Detailed Structure

### ğŸ“ airflow/
Airflow orchestration for workflow automation.

```
airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dbt_scheduled_transformations.py   # Twice daily dbt runs (2 AM & 2 PM)
â”‚   â”œâ”€â”€ egx_full_pipeline.py              # Complete daily pipeline (1 AM)
â”‚   â””â”€â”€ egx_unified_pipeline.py.old       # Legacy DAG (disabled)
â”œâ”€â”€ logs/                                  # Airflow execution logs
â””â”€â”€ plugins/                               # Custom Airflow plugins
```

**Active DAGs:**
- `dbt_scheduled_transformations`: Runs dbt staging â†’ marts â†’ tests â†’ docs
- `egx_full_pipeline`: Batch processing + dbt + data validation

### ğŸ“ docs/
Documentation and architecture diagrams.

```
docs/
â”œâ”€â”€ database/
â”‚   â””â”€â”€ DATABASE_REBUILD_SUMMARY.md        # Database rebuild documentation
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ DBT_COMPLETION_REPORT.md           # dbt implementation report
â”‚   â””â”€â”€ DBT_MODEL_UPDATE_SUMMARY.md        # Model changes documentation
â”œâ”€â”€ ARCHITECTURE.md                        # Complete architecture guide â­
â”œâ”€â”€ ARCHITECTURE_DIAGRAM.md                # Visual diagrams
â”œâ”€â”€ CLEANUP_SUMMARY.md                     # Git cleanup documentation
â”œâ”€â”€ EGX_INDICES.md                         # Egyptian Exchange indices info
â”œâ”€â”€ PROJECT_STRUCTURE.md                   # This file
â””â”€â”€ UPLOAD_TO_S3.md                        # S3 upload guide
```

### ğŸ“ egx_dw/
dbt project for data transformations (Silver â†’ Gold layers).

```
egx_dw/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/                           # Silver layer (5 models)
â”‚   â”‚   â”œâ”€â”€ stg_companies.sql
â”‚   â”‚   â”œâ”€â”€ stg_stock_prices_unified.sql
â”‚   â”‚   â”œâ”€â”€ stg_financials.sql
â”‚   â”‚   â”œâ”€â”€ stg_market_stats.sql
â”‚   â”‚   â”œâ”€â”€ stg_index_membership.sql
â”‚   â”‚   â””â”€â”€ schema.yml                     # Tests and documentation
â”‚   â”œâ”€â”€ marts/                             # Gold layer (7 models)
â”‚   â”‚   â”œâ”€â”€ gold_dim_company.sql           # Company dimension
â”‚   â”‚   â”œâ”€â”€ gold_fct_stock_daily_prices.sql # Price fact table
â”‚   â”‚   â”œâ”€â”€ gold_fct_index_performance.sql  # Index performance
â”‚   â”‚   â”œâ”€â”€ vw_company_performance_summary.sql
â”‚   â”‚   â”œâ”€â”€ vw_market_overview.sql
â”‚   â”‚   â”œâ”€â”€ vw_sector_analysis.sql
â”‚   â”‚   â”œâ”€â”€ vw_top_gainers_losers.sql
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â””â”€â”€ intermediate/                      # Temporary transformations
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ get_custom_schema.sql              # Schema naming logic
â”‚   â””â”€â”€ cleanup_old_schemas.sql            # Maintenance macro
â”œâ”€â”€ tests/                                 # Custom data quality tests
â”œâ”€â”€ dbt_project.yml                        # dbt configuration
â”œâ”€â”€ profiles.yml                           # Snowflake connection
â””â”€â”€ packages.yml                           # dbt dependencies
```

**Key Commands:**
```bash
dbt run            # Run all transformations
dbt test           # Run all tests (63 tests)
dbt docs generate  # Generate documentation
```

### ğŸ“ extract/
Data ingestion scripts for batch and streaming.

```
extract/
â”œâ”€â”€ aws/
â”‚   â””â”€â”€ connect_aws.py                     # AWS S3 connection utilities
â”œâ”€â”€ egxpy_streaming/
â”‚   â””â”€â”€ producer_kafka.py                  # Kafka producer (EGX API â†’ Kafka)
â”œâ”€â”€ streaming/
â”‚   â”œâ”€â”€ consumer_snowflake.py              # Kafka consumer (Kafka â†’ Snowflake)
â”‚   â”œâ”€â”€ consumer_kafka.py                  # Alternative consumer
â”‚   â””â”€â”€ producer.py                        # Alternative producer
â”œâ”€â”€ eodhd_api/                             # EODHD API integration (unused)
â”œâ”€â”€ kaggle/                                # Kaggle dataset downloads
â”œâ”€â”€ realtime/
â”‚   â””â”€â”€ consumer_influxdb.py               # InfluxDB consumer (metrics)
â””â”€â”€ batch_processor.py                     # S3 CSV â†’ Snowflake loader â­
```

**Main Scripts:**
- `egxpy_streaming/producer_kafka.py`: Fetches EGX data every 5 minutes
- `streaming/consumer_snowflake.py`: Writes to Snowflake in batches (100 records)
- `batch_processor.py`: Loads historical CSV files from S3

### ğŸ“ iam/
AWS IAM setup and security policies.

```
iam/
â”œâ”€â”€ bootstrap_admin.py                     # Initial admin user setup
â”œâ”€â”€ create_bucket.sh                       # S3 bucket creation
â”œâ”€â”€ create_team_users.sh                   # Team user provisioning
â”œâ”€â”€ setup_aws_iam.sh                       # Complete IAM setup
â”œâ”€â”€ egx_team_upload_policy.json            # S3 upload permissions
â”œâ”€â”€ snowflake-s3-read-policy.json          # Snowflake S3 integration
â””â”€â”€ snowflake-trust-policy.json            # Cross-account trust
```

### ğŸ“ infrastructure/
Service orchestration and Docker configuration.

```
infrastructure/
â””â”€â”€ docker/
    â”œâ”€â”€ grafana/                           # Grafana configuration
    â”œâ”€â”€ docker-compose.yml                 # All services (Kafka, Airflow, Grafana) â­
    â”œâ”€â”€ Dockerfile                         # Custom Airflow image
    â”œâ”€â”€ requirements.txt                   # Docker Python dependencies
    â”œâ”€â”€ setup.sh                           # Initial Docker setup
    â””â”€â”€ start_services.sh                  # Start all Docker services
```

**Services in docker-compose.yml:**
- Zookeeper (Kafka coordination)
- Kafka (port 9093)
- Airflow webserver (port 8081)
- Airflow scheduler
- Airflow init
- InfluxDB (metrics storage)
- Grafana (visualization, port 3000)

### ğŸ“ scripts/
Utility scripts for data loading and monitoring.

```
scripts/
â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ load_all_data_batch.py             # Batch load all data types
â”‚   â”œâ”€â”€ load_index_membership.py           # EGX30/70/100 membership
â”‚   â””â”€â”€ load_market_stats.py               # Market statistics
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ monitor_streaming.sh               # Pipeline health check â­
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ check_s3_bucket.py                 # S3 bucket validation
â”œâ”€â”€ bootstrap.sh                           # Initial project setup
â””â”€â”€ setup_s3_pipeline.sh                   # S3 pipeline configuration
```

**Key Script:**
- `monitoring/monitor_streaming.sh`: Health checks for Kafka, processes, Snowflake data

### ğŸ“ sql/
Database schemas and setup scripts.

```
sql/
â””â”€â”€ 00_create_database_from_scratch.sql    # Complete Snowflake schema setup
```

Creates:
- `EGX_OPERATIONAL_DB` database
- `OPERATIONAL` schema (6 tables)
- `DWH_SILVER` schema (staging)
- `DWH_GOLD` schema (analytics)

## Root Files

### Automation Scripts

| File | Purpose | Usage |
|------|---------|-------|
| `scripts/start_pipeline.sh` | Master startup (Kafka â†’ Streaming â†’ Airflow) | `./scripts/start_pipeline.sh` |
| `scripts/start_streaming.sh` | Streaming only | `./scripts/start_streaming.sh` |
| `scripts/stop_pipeline.sh` | Graceful shutdown | `./scripts/stop_pipeline.sh` |

### Configuration Files

| File | Purpose |
|------|---------|
| `requirements.txt` | Python dependencies (egxpy, kafka, snowflake, dbt) |
| `Makefile` | Build automation targets |
| `.env.example` | Environment variable template |
| `.gitignore` | Git exclusions |

### Log Files

| File | Generated By |
|------|--------------|
| `producer.log` | Kafka producer (EGX data fetching) |
| `consumer.log` | Kafka consumer (Snowflake writes) |

## Data Schemas

### Operational Layer
**Database:** `EGX_OPERATIONAL_DB.OPERATIONAL`

| Table | Records | Source | Description |
|-------|---------|--------|-------------|
| TBL_COMPANY | 249 | Batch + Streaming | Company master data |
| TBL_STOCK_PRICE | 130K+ | Batch + Streaming | OHLCV daily prices |
| TBL_FINANCIAL | 3.5K | Batch | Financial statements |
| TBL_MARKET_STAT | - | Batch | Market statistics |
| TBL_INDEX | 3 | Batch | EGX30, EGX70, EGX100 |
| TBL_INDEX_MEMBERSHIP | 176 | Batch | Company-index relationships |

### Silver Layer (Staging)
**Schema:** `EGX_OPERATIONAL_DB.DWH_SILVER`
- Cleaned, validated, type-converted data
- 5 staging models

### Gold Layer (Analytics)
**Schema:** `EGX_OPERATIONAL_DB.DWH_GOLD`
- Analytics-ready dimensional models
- 3 fact/dimension tables + 4 views

## File Naming Conventions

### Python Scripts
- `producer_*.py` - Data producers (fetch from APIs)
- `consumer_*.py` - Data consumers (write to databases)
- `load_*.py` - Batch loading scripts
- `check_*.py` - Validation utilities

### SQL Files
- `stg_*.sql` - Staging models (Silver layer)
- `gold_*.sql` - Analytics models (Gold layer)
- `vw_*.sql` - Views for analytics
- `00_*.sql` - Setup scripts (ordered)

### Shell Scripts
- `start_*.sh` - Startup scripts
- `stop_*.sh` - Shutdown scripts
- `setup_*.sh` - Configuration scripts
- `monitor_*.sh` - Health check scripts

## Key Paths for Configuration

### Environment Variables
```bash
# Main environment file
egx_dw/.env

# Variables needed:
SNOWFLAKE_ACCOUNT=...
SNOWFLAKE_USER=...
SNOWFLAKE_PASSWORD=...
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=EGX_OPERATIONAL_DB
SNOWFLAKE_ROLE=SYSADMIN
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
```

### Docker Volumes
```yaml
# Airflow DAGs mount
../../airflow/dags:/opt/airflow/dags

# Logs mount
../../airflow/logs:/opt/airflow/logs
```

### dbt Profiles
```yaml
# Location: egx_dw/profiles.yml
egx_dw:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      # ... other Snowflake configs
```

## Navigation Guide

### To Work On...

**Streaming Pipeline:**
```bash
cd extract/egxpy_streaming/        # Producer
cd extract/streaming/              # Consumer
./start_streaming.sh               # Start both
```

**dbt Transformations:**
```bash
cd egx_dw
source ../.venv-aws/bin/activate
dbt run
```

**Airflow DAGs:**
```bash
cd airflow/dags/
# Edit dbt_scheduled_transformations.py or egx_full_pipeline.py
docker compose -f infrastructure/docker/docker-compose.yml restart airflow-scheduler
```

**Monitoring:**
```bash
./scripts/monitoring/monitor_streaming.sh
tail -f producer.log consumer.log
```

**Documentation:**
```bash
cd docs/
# Edit ARCHITECTURE.md or other docs
```

## Quick Reference

### Start Everything
```bash
./scripts/start_pipeline.sh
```

### Stop Everything
```bash
./scripts/stop_pipeline.sh
```

### Check Health
```bash
./scripts/monitoring/monitor_streaming.sh
```

### View Logs
```bash
tail -f producer.log consumer.log
```

### Airflow UI
http://localhost:8081 (admin/admin)

### Grafana
http://localhost:3000 (admin/admin)

---

**Last Updated:** December 9, 2025  
**Total Files:** 100+  
**Total Lines of Code:** ~15,000+
